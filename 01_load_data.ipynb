{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 01 - Loading data\n",
    "\n",
    "The following notebook will have 2 objectives:\n",
    "1. Give general outlines of the projects\n",
    "2. Load the dataset and extract useful features.\n",
    "\n",
    "#### Objectives\n",
    "\n",
    "The goal is to build a simple neural network to predict fake news\n",
    "based on various news articles. While the data presented here can be used\n",
    "with top-of-the-line algorithms, most of the work here is proof of concept\n",
    "on deep learning. The major road-block is hardware. It takes massive amount\n",
    "of processing power to clean and run these models.\n",
    "\n",
    "With that in mind, only a small subsets of the data will be utilized.\n",
    "Moreover, only two categories of news are retained: fake and reliable.\n",
    "\n",
    "#### Pipelines\n",
    "\n",
    "1. Load and extract desired features and rows\n",
    "2. Get a small sample from the data (2000 articles):\n",
    "    - Clean and tokenize\n",
    "    - Generate Term Document Sparse Matrix.\n",
    "3. Fit model with neural network.\n",
    "4. Model evaluations\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "Credits: several27 renatosc\n",
    "\n",
    "The following [dataset](https://github.com/several27/FakeNewsCorpus)\n",
    "will be used. This is an open-sourced dataset comprised of millions\n",
    "of news article with label from fake, religious, satire,...to reliable.\n",
    "\n",
    "Note: dataset's size is over 3.5 Gb.\n",
    "\n",
    "Labels definitions:\n",
    "1. fake: Sources that entirely fabricate information, disseminate deceptive\n",
    "content, or grossly distort actual news reports.\n",
    "2. reliable: Sources that circulate news and information in a manner consistent with traditional and ethical practices in journalism (Remember: even credible sources sometimes rely on clickbait-style headlines or occasionally make mistakes. No news organization is perfect, which is why a healthy news diet consists of multiple sources of information).\n",
    "\n",
    "#### Loading data\n",
    "\n",
    "Contents and titles are the two features that will be used to predict\n",
    "news type - fake or reliable.\n",
    "\n",
    "Reading in the data directly is impossible due to the size. Thus, an\n",
    "iterator is used instead. The chosen chunksize is 5000 which works best\n",
    "with the hardware provided.\n",
    "\n",
    "For each chunk, extract only content and type columns. Afterward, filter\n",
    "rows with type labeled fake or reliable. All chunks are concatenated\n",
    "as one dataset - saved for used later."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Increase csv file limit\n",
    "import csv\n",
    "csv.field_size_limit(2000000)\n",
    "\n",
    "# Data path for all saved data\n",
    "data_path = 'D:\\\\PycharmProjects\\\\springboard\\\\data\\\\'\n",
    "file_name = 'news_cleaned_2018_02_13.csv'\n",
    "\n",
    "# Chunk size\n",
    "chunksize = 5000\n",
    "\n",
    "# Only extract content and type columns\n",
    "use_cols = ['type', 'content']\n",
    "use_rows = ['fake', 'reliable']\n",
    "\n",
    "# Read data in chunk\n",
    "data_iterator = pd.read_csv(f'{data_path}{file_name}',\n",
    "                    usecols=use_cols,\n",
    "                    chunksize=chunksize,\n",
    "                    encoding='ISO-8859-1',\n",
    "                    index_col=False,\n",
    "                    engine='python')\n",
    "\n",
    "# Read each chunk and extract desired features and rows\n",
    "chunk_list = []\n",
    "for chunk in data_iterator:\n",
    "    filtered_chunk = chunk[chunk.type.isin(use_rows)]\n",
    "    chunk_list.append(filtered_chunk)\n",
    "\n",
    "# Joining and save the cleaned data for use later\n",
    "filtered_data = pd.concat(chunk_list)\n",
    "filtered_data.to_csv(\"D:\\\\PycharmProjects\\\\springboard\\\\data\\\\news_clean_1.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}